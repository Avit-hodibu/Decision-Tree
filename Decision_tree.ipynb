{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpm027kj7xH4UmfkuWeNo0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Avit-hodibu/Decision-Tree/blob/main/Decision_tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision tree is cutting the line in pieceies. Divide each to different group\n"
      ],
      "metadata": {
        "id": "U2NMQWKuP5BH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pseudo Code**\n",
        "\n",
        "- Begin with your training dataset, which should have some feature variables and classifiction or regression output.\n",
        "- determinr the best feature in the dataset to split the data on; more on how define best feature later(root node)\n",
        "- split the data into subsets that contain the correct values for this best feature. This splitting basically define a node on the tree i.e. each node is a splitting point based on a certain feature from our data.\n",
        "- Recursively generate new tree nodes by using subset of data created from step 3\n",
        "\n"
      ],
      "metadata": {
        "id": "xO7lVx6_SZHw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "proframatically = t is nothing but giant structure of if-else condition.\n",
        "\n",
        "Mathamatically = It use hyperplanes which run parallel to any one of the axes to cut your coordinate system into hyper cuboids. (divider of data)"
      ],
      "metadata": {
        "id": "h5qyZ3o2Trzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Terminology:**\n",
        "\n",
        "Rootnode=  start node\n",
        "\n",
        "splitting = from where data split\n",
        "\n",
        "decision node = neither leaf nor root\n",
        "\n",
        "leaf node= last node  \n",
        "\n",
        "branch/ subtree"
      ],
      "metadata": {
        "id": "EtYvoxIXUrPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adv\n",
        "- intuitive and easy to understand\n",
        "- minimal data preparation is required\n",
        "- the cost of using the tree for inference is logarithmic in the number of data points used to train the tree\n",
        "\n",
        "Dis:\n",
        "- overfitting\n",
        "- prone to errors for imbalanced datasets\n",
        "\n"
      ],
      "metadata": {
        "id": "M-Qr0xUiMClc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CART- classification and Regression Trees: the logic of decision trees can also be applied to regression problems, hence the name CART.\n",
        "\n"
      ],
      "metadata": {
        "id": "XsKGH_gNPo-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Entropy**: the measure of disorder. Or you can also call it the measure of purity / impurity.\n",
        "\n",
        "More knowledge less entropy\n",
        "\n",
        "for eg: In ice, water and vapour, vapour has high entropy and ice has least.\n",
        "\n",
        "To calculate entropy:\n",
        "\n",
        "E(S)= E i= 1 to c  (-pi log2 pi )\n",
        "\n",
        "where pi is simply the frequentist probability of an element/class i in our data.\n",
        "\n",
        "For eg: in 10 data, we have 3 NO, 2 YES, 3 MAYBE. then entropy:\n",
        "\n",
        "H(d)= - Pylog2(Py)-Pnlog2(Pn)-Pmlog2(Pm)\n",
        "H(d)= -2/8log2(2/8)-3/8log2(3/8)-3/8log2(3/8)\n",
        "H(d)= 1.56\n",
        "\n",
        "Observation:\n",
        "- more the uncertainty more is entropy\n",
        "- for 2 class problem the min entropy = 0 and max =1\n",
        "- for more than 2 class min = 0 but max is greater that 1\n",
        "- both log2 and loge can be used\n",
        "\n",
        "Entropy For continous variables:\n",
        "- KDE or histogram plot\n",
        "- less peaked dataset has higher entropy dataset\n"
      ],
      "metadata": {
        "id": "sqI64MR3P-tV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Infromation Gain**\n",
        "It is a metric used to train DT. Specifically, this metric measures the quality of a split.\n",
        "\n",
        "The information gain is based on the decrease in entropy after a data-set is split on an attribute. Constructing a decision tree is all about finding attribut that returns the highest information gain.\n",
        "\n",
        "IG= E(parent) - (Weighted Average) * E(Children)\n",
        "\n"
      ],
      "metadata": {
        "id": "inu3L3jfYfsb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Entropy of Parent\n",
        "\n",
        "Step 2: Divide  and calulate entropy for children\n",
        "\n",
        "Step 3: Calculate weighted Entropy of children\n",
        "\n",
        "Weighted Entropy = (number of row of children / total number of rows) * entropy of children\n",
        "\n",
        "step 4: Calculate Information Gain\n",
        "\n",
        "step 5: Calculate IG for all the columns\n",
        "\n",
        "column has the highest IG will be selected that column to split the data.\n",
        "\n",
        "Step 6: Find IG recursively\n",
        "\n",
        "DT then applies a recurive greedy search algo in top bottom fashion to find IG at every level of the tree\n",
        "\n",
        "Once a leaf node is reached (Entropy =0), no more splitting is done.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fjZpANY0aTRo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXU9R4bBPbs9"
      },
      "outputs": [],
      "source": []
    }
  ]
}